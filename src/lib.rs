use std::rc::Rc;

use api::{ByteswiseComparator, Comparator};

mod api;
mod config;
mod db;
mod journal;
pub mod memdb;
mod table;
mod util;

#[derive(Clone)]
pub struct Options {
    // Number of keys between restart points for delta encoding of keys.
    // This parameter can be changed dynamically.  Most clients should
    // leave this parameter alone.
    block_restart_interval: usize,

    // Compress blocks using the specified compression algorithm.  This
    // parameter can be changed dynamically.
    //
    // Default: kSnappyCompression, which gives lightweight but fast
    // compression.
    //
    // Typical speeds of kSnappyCompression on an Intel(R) Core(TM)2 2.4GHz:
    //    ~200-500MB/s compression
    //    ~400-800MB/s decompression
    // Note that these speeds are significantly faster than most
    // persistent storage speeds, and therefore it is typically never
    // worth switching to kNoCompression.  Even if the input data is
    // incompressible, the kSnappyCompression implementation will
    // efficiently detect that and will switch to uncompressed mode.
    compression: CompressionType,

    // Comparator used to define the order of keys in the table.
    // Default: a comparator that uses lexicographic byte-wise ordering
    //
    // REQUIRES: The client must ensure that the comparator supplied
    // here has the same name and orders keys *exactly* the same as the
    // comparator provided to previous open calls on the same DB.
    comparator: Rc<dyn Comparator>,

    // If true, the implementation will do aggressive checking of the
    // data it is processing and will stop early if it detects any
    // errors.  This may have unforeseen ramifications: for example, a
    // corruption of one DB entry may cause a large number of entries to
    // become unreadable or for the entire DB to become unopenable.
    paranoid_checks: bool,

    // Any internal progress/error information generated by the db will
    // be written to info_log if it is non-null, or to a file stored
    // in the same directory as the DB contents if info_log is null.
    info_log: Option<Rc<dyn Logger>>,

    // -------------------
    // Parameters that affect performance

    // Amount of data to build up in memory (backed by an unsorted log
    // on disk) before converting to a sorted on-disk file.
    //
    // Larger values increase performance, especially during bulk loads.
    // Up to two write buffers may be held in memory at the same time,
    // so you may wish to adjust this parameter to control memory usage.
    // Also, a larger write buffer will result in a longer recovery time
    // the next time the database is opened.
    write_buffer_size: usize,

    // Number of open files that can be used by the DB.  You may need to
    // increase this if your database has a large working set (budget
    // one open file per 2MB of working set).
    max_open_files: usize,

    // Approximate size of user data packed per block.  Note that the
    // block size specified here corresponds to uncompressed data.  The
    // actual size of the unit read from disk may be smaller if
    // compression is enabled.  This parameter can be changed dynamically.
    block_size: usize,

    // Leveldb will write up to this amount of bytes to a file before
    // switching to a new one.
    // Most clients should leave this parameter alone.  However if your
    // filesystem is more efficient with larger files, you could
    // consider increasing the value.  The downside will be longer
    // compactions and hence longer latency/performance hiccups.
    // Another reason to increase this parameter might be when you are
    // initially populating a large database.
    max_file_size: usize,

    // Control over blocks (user data is stored in a set of blocks, and
    // a block is the unit of reading from disk).

    // If non-null, use the specified cache for blocks.
    // If null, leveldb will automatically create and use an 8MB internal cache.
    block_cache: Option<Rc<dyn Cache>>,
}

pub const NUM_NON_TABLE_CACHE_FILES: usize = 10;

impl Options {
    fn default() -> Self {
        Options {
            block_restart_interval: 16,
            block_size: 4 * 1024,
            compression: CompressionType::SnappyCompression,
            comparator: Rc::new(ByteswiseComparator {}),
            paranoid_checks: false,
            max_open_files: 1000,
            write_buffer_size: 4 * 1024 * 1024,
            max_file_size: 2 * 1024 * 1024,
            info_log: None,
            block_cache: None,
        }
    }
}

// DB contents are stored in a set of blocks, each of which holds a
// sequence of key,value pairs.  Each block may be compressed before
// being stored in a file.  The following enum describes which
// compression method (if any) is used to compress a block.
#[derive(Copy, Clone, Debug, Eq, PartialEq)]
pub enum CompressionType {
    // NOTE: do not change the values of existing entries, as these are
    // part of the persistent format on disk.
    NoCompression = 0x0,
    SnappyCompression = 0x1,
}

impl From<u8> for CompressionType {
    fn from(c: u8) -> Self {
        match c {
            0x0 => CompressionType::NoCompression,
            0x01 => CompressionType::SnappyCompression,
            _ => CompressionType::NoCompression,
        }
    }
}

// A file abstraction for sequential writing.  The implementation
// must provide buffering since callers may append small fragments
// at a time to the file.
pub trait WritableFile {
    fn append(&mut self, data: &[u8]) -> api::Result<()>;
    fn close(&mut self) -> api::Result<()>;
    fn flush(&mut self) -> api::Result<()>;
    fn sync(&mut self) -> api::Result<()>;
}

// Destroy the contents of the specified database.
// Be very careful using this method.
//
// Note: For backwards compatibility, if DestroyDB is unable to list the
// database files, Status::OK() will still be returned masking this failure.
/* fn DestroyDB(name:&String, options:&Options<C>) {

} */

pub trait RandomAccessFile {
    // Read up to "n" bytes from the file starting at "offset".
    // "scratch[0..n-1]" may be written by this routine.  Sets "*result"
    // to the data that was read (including if fewer than "n" bytes were
    // successfully read).  May set "*result" to point at data in
    // "scratch[0..n-1]", so "scratch[0..n-1]" must be live when
    // "*result" is used.  If an error was encountered, returns a non-OK
    // status.
    //
    // Safe for concurrent use by multiple threads.
    fn read(&self, offset: usize, n: usize, dst: &mut Vec<u8>) -> api::Result<usize>;
}

pub trait SequentialFile {
    // Read up to "n" bytes from the file.  "scratch[0..n-1]" may be
    // written by this routine.  Sets "*result" to the data that was
    // read (including if fewer than "n" bytes were successfully read).
    // May set "*result" to point at data in "scratch[0..n-1]", so
    // "scratch[0..n-1]" must be live when "*result" is used.
    // If an error was encountered, returns a non-OK status.
    //
    // REQUIRES: External synchronization
    fn read(
        &mut self,
        n: usize,
        result: &mut Vec<u8>,
        scratch: &mut Vec<u8>,
    ) -> std::io::Result<()>;

    // Skip "n" bytes from the file. This is guaranteed to be no
    // slower that reading the same data, but may be faster.
    //
    // If end of file is reached, skipping will stop at the end of the
    // file, and Skip will return OK.
    //
    // REQUIRES: External synchronization
    fn skip(&mut self, n: usize) -> std::io::Result<()>;
}

pub trait Logger {}

// Create a new cache with a fixed size capacity.  This implementation
// of Cache uses a least-recently-used eviction policy.
pub trait Cache {}
